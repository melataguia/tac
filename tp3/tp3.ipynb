{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import string\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from pprint import pprint\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from unidecode import unidecode\n",
    "from timeout_decorator import timeout, TimeoutError\n",
    "from threading import Timer\n",
    "from yake import KeywordExtractor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/txt/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DECADE = '1900'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f for f in sorted(os.listdir(data_path)) if f\"_{DECADE[:-1]}\" in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choisir de la décennie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de fichiers\n",
    "files[:5]\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [open(data_path + f, \"r\", encoding=\"utf-8\").read() for f in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de textes\n",
    "texts[0][:400]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectoriser les documents à l'aide de TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une fonction de pré-traitement\n",
    "def preprocessing(text, stem=True):\n",
    "    \"\"\" Tokenize text and remove punctuation \"\"\"\n",
    "    text = text.translate(string.punctuation)\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instancier le modèle TF-IDF avec ses arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=preprocessing,\n",
    "    stop_words=stopwords.words('french'),\n",
    "    max_df=0.5,\n",
    "    min_df=0.1,\n",
    "    lowercase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construire la matrice de vecteurs à l'aide de la fonction `fit_transform`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectors = vectorizer.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Détail de la matrice\n",
    "tfidf_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imprimer le vecteur tf-IDF du premier document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(\n",
    "    tfidf_vectors[0].toarray()[0],\n",
    "    index=vectorizer.get_feature_names_out()\n",
    "    ).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprendre les vecteurs et leurs \"distances\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine([1, 2, 3], [1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine([1, 2, 3], [1, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine([1, 2, 3], [2, 2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests sur nos documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_array = tfidf_vectors.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vecteur du document 0\n",
    "tfidf_array[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vecteur du document 1\n",
    "tfidf_array[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine(tfidf_array[0], tfidf_array[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appliquer un algorithme de clustering sur les vecteurs TF-IDF des documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour en savoir plus sur le KMeans clustering :\n",
    "- https://medium.com/dataseries/k-means-clustering-explained-visually-in-5-minutes-b900cc69d175"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définir un nombre de clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLUSTERS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instancier le modèle K-Means et ses arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_model = KMeans(n_clusters=N_CLUSTERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appliquer le clustering à l'aide de la fonction `fit_predict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = km_model.fit_predict(tfidf_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = collections.defaultdict(list)\n",
    "\n",
    "for idx, label in enumerate(clusters):\n",
    "    clustering[label].append(files[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(dict(clustering))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualiser le contenu des differents textes de chaque clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecture du  contenu des clusters avec une limite de temps\n",
    "def read_file_content(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "# gestion  du  délai d'exécution\n",
    "def timeout_handler():\n",
    "    raise TimeoutError(\"Function execution timed out\")\n",
    "\n",
    "# Exploration des données brutes de chaque cluster\n",
    "for cluster_label, files_in_cluster in clustering.items():\n",
    "    print(f\"Cluster {cluster_label} :\")\n",
    "    #pprint(files_in_cluster)\n",
    "    print(\"\\n\")\n",
    "\n",
    "   \n",
    "    content_list = []\n",
    "    for txt in files_in_cluster:\n",
    "        timer = Timer(5, timeout_handler)  \n",
    "        try:\n",
    "            timer.start()\n",
    "            content = read_file_content(os.path.join(data_path, txt))\n",
    "            content_list.append(content)\n",
    "        except TimeoutError:\n",
    "            print(f\"Reading {txt} took too long. Skipping.\")\n",
    "            continue\n",
    "        finally:\n",
    "            timer.cancel()\n",
    "\n",
    "    # Afficher le contenu de chaque texte dans le cluster\n",
    "    for txt in files_in_cluster:\n",
    "        try:\n",
    "            content = read_file_content(os.path.join(data_path, txt))\n",
    "            print(f\"Text in Cluster {cluster_label}:\")\n",
    "            print(content)\n",
    "            print(\"\\n\")\n",
    "        except TimeoutError:\n",
    "            print(f\"Reading {txt} took too long. Skipping.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stockage du contenu de chaque cluster dans une liste\n",
    "all_cluster_content = []\n",
    "\n",
    "# Exploration des données brutes de chaque cluster\n",
    "for cluster_label, files_in_cluster in clustering.items():\n",
    "   \n",
    "\n",
    "    \n",
    "    cluster_content = \"\"\n",
    "    for txt in files_in_cluster:\n",
    "        try:\n",
    "            content = read_file_content(os.path.join(data_path, txt))\n",
    "            cluster_content += content + \"\\n\"  \n",
    "        except TimeoutError:\n",
    "            print(f\"Reading {txt} took too long. Skipping.\")\n",
    "\n",
    "    # Ajouter le contenu du cluster au corpus global\n",
    "    all_cluster_content.append(cluster_content)\n",
    "\n",
    "# Afficher le contenu de chaque cluster\n",
    "for cluster_label, cluster_content in zip(clustering.keys(), all_cluster_content):\n",
    "    print(f\"Cluster {cluster_label} - All Content:\")\n",
    "    print(cluster_content)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction de keywords des differents clusters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keywords pour chaque clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kw_extractor = KeywordExtractor()\n",
    "\n",
    "all_cluster_keywords = []\n",
    "\n",
    "for cluster_label, files_in_cluster in clustering.items():\n",
    "  \n",
    "    cluster_content = \"\"\n",
    "    for txt in files_in_cluster:\n",
    "        try:\n",
    "            content = read_file_content(os.path.join(data_path, txt))\n",
    "            cluster_content += content + \"\\n\"  # Concaténer le contenu des textes avec un saut de ligne\n",
    "        except TimeoutError:\n",
    "            print(f\"Reading {txt} took too long. Skipping.\")\n",
    "\n",
    "    # Extraction des mots-clés du contenu du cluster\n",
    "    cluster_keywords = kw_extractor.extract_keywords(cluster_content)\n",
    "\n",
    "   \n",
    "    all_cluster_keywords.append(cluster_keywords)\n",
    "\n",
    "# Affichage des mots-clés de chaque cluster\n",
    "for cluster_label, cluster_keywords in zip(clustering.keys(), all_cluster_keywords):\n",
    "    print(f\"Cluster {cluster_label} - Keywords:\")\n",
    "    pprint(cluster_keywords)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_extractor = KeywordExtractor()\n",
    "\n",
    "for cluster_label, files_in_cluster in clustering.items():\n",
    "    \n",
    "\n",
    "    # Stocker le contenu de chaque texte dans le cluster\n",
    "    cluster_content = \"\"\n",
    "    for txt in files_in_cluster:\n",
    "        try:\n",
    "            content = read_file_content(os.path.join(data_path, txt))\n",
    "            cluster_content += content + \"\\n\"  # Concaténer le contenu des textes avec un saut de ligne\n",
    "        except TimeoutError:\n",
    "            print(f\"Reading {txt} took too long. Skipping.\")\n",
    "\n",
    "    # Extraire les mots-clés du contenu du cluster\n",
    "    cluster_keywords = kw_extractor.extract_keywords(cluster_content)\n",
    "\n",
    "    # Ajouter les mots-clés du cluster à la liste globale\n",
    "    all_cluster_keywords.extend(cluster_keywords)\n",
    "\n",
    "# Afficher les mots-clés de tous les clusters\n",
    "print(\"Keywords in All Clusters:\")\n",
    "kept_keywords = []\n",
    "for kw, score in all_cluster_keywords:\n",
    "    words = kw.split()\n",
    "    if len(words) == 2:\n",
    "        kept_keywords.append(kw)\n",
    "\n",
    "print(f\"Keywords: {', '.join(kept_keywords)}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualiser les clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse de nos clusters avec wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### definition de nos stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste de Stopwords enrichis (Idem que dans s1)\n",
    "sw = stopwords.words(\"french\")\n",
    "sw += [\"abord\", \"ailleurs\", \"ainsi\", \"alors\", \"après\", \"avant\", \"avoir\", \"bien\", \"bientôt\",\n",
    "    \"car\", \"ce\", \"ceci\", \"cela\", \"ces\", \"cet\", \"cette\", \"comme\", \"contre\", \"dans\",\n",
    "    \"depuis\", \"dire\", \"doit\", \"donc\", \"elle\", \"encore\", \"enfin\", \"ensuite\", \"entre\", \"être\",\n",
    "    \"fait\", \"faire\", \"faut\", \"hormis\", \"ici\", \"il\", \"ils\", \"jusqu'à\", \"les\", \"leur\", \"là\",\n",
    "    \"ma\", \"mais\", \"mes\", \"moins\", \"mon\", \"ne\", \"non\", \"nous\", \"on\", \"ou\", \"ou bien\", \"par\",\n",
    "    \"parce que\", \"parfois\", \"pas\", \"pendant\", \"peut\", \"plus\", \"plutôt\", \"pour\", \"puis\",\n",
    "    \"quand\", \"quant à\", \"sans\", \"sa\", \"se\", \"ses\", \"son\", \"sous\", \"tant pis\", \"tandis que\",\n",
    "    \"trois\", \"tôt ou tard\", \"toutefois\", \"toutes\", \"tu\", \"tous\", \"trois\", \"trois\", \"trois\",\n",
    "    \"voilà\", \"à\", \"très\"]\n",
    "sw = set(sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ecriture dans un fichier temporaire des differents clusters pour l'analyse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_path = '../data/tmp'\n",
    "if not os.path.exists(temp_path):\n",
    "    os.mkdir(temp_path)\n",
    "\n",
    "# Exploration des données brutes de chaque cluster\n",
    "for cluster_label, files_in_cluster in clustering.items():\n",
    "  \n",
    "\n",
    "    # Stocker le contenu de chaque texte dans le cluster\n",
    "    cluster_content = \"\"\n",
    "    for txt in files_in_cluster:\n",
    "        try:\n",
    "            content = read_file_content(os.path.join(data_path, txt))\n",
    "            cluster_content += content + \"\\n\"  # Concaténer le contenu des textes avec un saut de ligne\n",
    "        except TimeoutError:\n",
    "            print(f\"Reading {txt} took too long. Skipping.\")\n",
    "\n",
    "    # Écrire le contenu du cluster dans un fichier temporaire\n",
    "    with open(os.path.join(temp_path, f'cluster_{cluster_label}.txt'), 'w', encoding='utf-8') as f:\n",
    "        f.write(cluster_content)\n",
    "\n",
    "print(\"Content of all clusters has been written to temporary files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemin du fichier qui regroupera le contenu de tous les clusters\n",
    "output_path = os.path.join(temp_path, 'cluster_2.txt')\n",
    "\n",
    "# Imprimer le contenu du fichier et constater les \"déchets\"\n",
    "with open(output_path, 'r', encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def clean_text(content, folder=None, stop_words=None):\n",
    "    # Si le dossier n'est pas spécifié, utilisez le chemin par défaut\n",
    "    folder = folder or '.'\n",
    "\n",
    "    # Utilisez une liste de stop-words fournie ou une liste vide par défaut\n",
    "    sw = stop_words or []\n",
    "\n",
    "    if folder is None:\n",
    "        input_path = \"cluster_2.txt\"\n",
    "        output_path = \"all_clusters_clean.txt\"\n",
    "    else:\n",
    "        input_path = f\"{folder}/cluster_2.txt\"\n",
    "        output_path = f\"{folder}/all_clusters_clean.txt\"\n",
    "\n",
    "    # Créez le fichier de sortie pour écrire le texte nettoyé\n",
    "    with open(output_path, \"w\", encoding='utf-8') as output:\n",
    "        # Utilisez le contenu du cluster au lieu de lire depuis un fichier\n",
    "        text = cluster_content\n",
    "\n",
    "        # Tokenisez les mots en utilisant nltk\n",
    "        words = nltk.wordpunct_tokenize(text)\n",
    "\n",
    "        # Filtrez les mots en fonction des critères définis\n",
    "        kept = [w.upper() for w in words if len(w) > 2 and w.isalpha() and w.lower() not in sw]\n",
    "\n",
    "        # Joignez les mots filtrés pour former une chaîne\n",
    "        kept_string = \" \".join(kept)\n",
    "\n",
    "        # Écrivez la chaîne nettoyée dans le fichier de sortie\n",
    "        output.write(kept_string)\n",
    "\n",
    "    return f'Output has been written in {output_path}!'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text(content, folder=temp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(temp_path, f'all_clusters_clean.txt'), 'r', encoding='utf-8') as f:\n",
    "    after = f.read()\n",
    "\n",
    "print(after[:1000]) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Compter la fréquence des mots dans le texte nettoyé\n",
    "frequencies = Counter(after.split())\n",
    "\n",
    "# Afficher les 10 mots les plus fréquents\n",
    "print(frequencies.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Réduire les vecteurs à 2 dimensions à l'aide de l'algorithme PCA\n",
    "Cette étape est nécessaire afin de visualiser les documents dans un espace 2D\n",
    "\n",
    "https://fr.wikipedia.org/wiki/Analyse_en_composantes_principales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "reduced_vectors = pca.fit_transform(tfidf_vectors.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_vectors[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Générer le plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = reduced_vectors[:, 0]\n",
    "y_axis = reduced_vectors[:, 1]\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "scatter = plt.scatter(x_axis, y_axis, s=100, c=clusters)\n",
    "\n",
    "# Ajouter les centroïdes\n",
    "centroids = pca.transform(km_model.cluster_centers_)\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1],  marker = \"x\", s=100, linewidths = 2, color='black')\n",
    "\n",
    "# Ajouter la légende\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=set(clusters), title=\"Clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = reduced_vectors[:, 0]\n",
    "y_axis = reduced_vectors[:, 1]\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "scatter = plt.scatter(x_axis, y_axis, s=100, c=clusters)\n",
    "\n",
    "# Ajouter les centroïdes\n",
    "centroids = pca.transform(km_model.cluster_centers_)\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1],  marker = \"x\", s=100, linewidths = 2, color='black')\n",
    "\n",
    "# Ajouter la légende\n",
    "legend = plt.legend(handles=scatter.legend_elements()[0], labels=set(clusters), title=\"Clusters\")\n",
    "\n",
    "# Explorer chaque cluster\n",
    "for cluster_label in set(clusters):\n",
    "    cluster_points = reduced_vectors[clusters == cluster_label]\n",
    "    \n",
    "    # Afficher les points du cluster\n",
    "    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], s=50, label=f'Cluster {cluster_label}', alpha=0.5)\n",
    "\n",
    "plt.title('Clusters and Centroids')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster_label in set(clusters):\n",
    "    cluster_points = reduced_vectors[clusters == cluster_label]\n",
    "    \n",
    "    # Afficher les données brutes du cluster\n",
    "    print(f\"Cluster {cluster_label} - Raw Data:\")\n",
    "    print(cluster_points)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WORD_EMBEDDING "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création d'un objet qui *streame* les lignes d'un fichier pour économiser de la RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySentences(object):\n",
    "    \"\"\"Tokenize and Lemmatize sentences\"\"\"\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "\n",
    "    def __iter__(self):\n",
    "        for line in open(self.filename, encoding='utf-8', errors=\"backslashreplace\"):\n",
    "            yield [unidecode(w.lower()) for w in wordpunct_tokenize(line)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = f\"../data/sents.txt\"\n",
    "sentences = MySentences(infile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Détection des bigrams\n",
    "\n",
    "Article intéressant sur le sujet : https://towardsdatascience.com/word2vec-for-phrases-learning-embeddings-for-more-than-one-word-727b6cf723cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_phrases = Phrases(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'object `phrases` peut être vu comme un large dictionnaire d'expressions multi-mots associées à un score, le *PMI-like scoring*. Ce dictionnaire est construit par un apprentissage sur base d'exemples.\n",
    "Voir les références ci-dessous :\n",
    "- https://arxiv.org/abs/1310.4546\n",
    "- https://en.wikipedia.org/wiki/Pointwise_mutual_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(bigram_phrases.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il contient de nombreuses clés qui sont autant de termes observés dans le corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bigram_phrases.vocab.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prenons une clé au hasard :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_ = list(bigram_phrases.vocab.keys())[144]\n",
    "print(key_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le dictionnaire indique le score de cette coocurrence :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_phrases.vocab[key_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion des `Phrases` en objet `Phraser`\n",
    "\n",
    "`Phraser` est un alias pour `gensim.models.phrases.FrozenPhrases`, voir ici https://radimrehurek.com/gensim/models/phrases.html.\n",
    "\n",
    "Le `Phraser` est une version *light* du `Phrases`, plus optimale pour transformer les phrases en concaténant les bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_phraser = Phraser(phrases_model=bigram_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction des trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_phrases = Phrases(bigram_phraser[sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_phraser = Phraser(phrases_model=trigram_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création d'un corpus d'unigrams, bigrams, trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(trigram_phraser[bigram_phraser[sentences]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrainement d'un modèle Word2Vec sur ce corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = Word2Vec(\n",
    "    corpus, # On passe le corpus de ngrams que nous venons de créer\n",
    "    vector_size=32, # Le nombre de dimensions dans lesquelles le contexte des mots devra être réduit, aka. vector_size\n",
    "    window=5, # La taille du \"contexte\", ici 5 mots avant et après le mot observé\n",
    "    min_count=5, # On ignore les mots qui n'apparaissent pas au moins 5 fois dans le corpus\n",
    "    workers=4, # Permet de paralléliser l'entraînement du modèle en 4 threads\n",
    "    epochs=5 # Nombre d'itérations du réseau de neurones sur le jeu de données pour ajuster les paramètres avec la descente de gradient, aka. epochs.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Définir différentes valeurs pour window et min_count\n",
    "window_values = [3, 5, 7]\n",
    "min_count_values = [3, 5, 10]\n",
    "\n",
    "# Des essais-erreurs\n",
    "best_model = None\n",
    "best_score = float('-0.01')  \n",
    "\n",
    "for window_size in window_values:\n",
    "    for min_count_value in min_count_values:\n",
    "        print(f\"Entraînement avec window={window_size}, min_count={min_count_value}\")\n",
    "        \n",
    "        # Entraînement du modèle\n",
    "        model = Word2Vec(\n",
    "            corpus,\n",
    "            vector_size=32,\n",
    "            window=window_size,\n",
    "            min_count=min_count_value,\n",
    "            workers=4,\n",
    "            epochs=5\n",
    "        )\n",
    "\n",
    "        score = len(model.wv.key_to_index)\n",
    "\n",
    "        print(f\"Score obtenu : {score}\\n\")\n",
    "\n",
    "        # Comparaison avec le meilleur modèle précédent\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_model = model\n",
    "\n",
    "print(\"Entraînement terminé.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sauver le modèle dans un fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = f\"../data/newspapers.model\"\n",
    "model.save(outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorer le modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Charger le modèle en mémoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"../data/newspapers.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imprimer le vecteur d'un terme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv[\"ministre\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculer la similarité entre deux termes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity(\"ministre\", \"roi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chercher les mots les plus proches d'un terme donné"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(\"ministre\", topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faire des recherches complexes à travers l'espace vectoriel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.most_similar(positive=['paris', 'londres'], negative=['belgique']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution de 06 exemples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "similarity_example_1 = model.wv.similarity('paris', 'france')\n",
    "print(f\"Similarité entre 'paris' et 'france': {similarity_example_1}\")\n",
    "\n",
    "\n",
    "similarity_example_2 = model.wv.similarity('londres', 'royaume_uni')\n",
    "print(f\"Similarité entre 'londres' et 'royaume_uni': {similarity_example_2}\")\n",
    "\n",
    "\n",
    "similarity_example_3 = model.wv.similarity('belgique', 'france')\n",
    "print(f\"Similarité entre 'belgique' et 'france': {similarity_example_3}\")\n",
    "\n",
    "\n",
    "most_similar_example_1 = model.wv.most_similar(\"ministre\", topn=3)\n",
    "print(f\"Mots similaires à 'ministre': {most_similar_example_1}\")\n",
    "\n",
    "\n",
    "most_similar_example_2 = model.wv.most_similar(\"parlement\", topn=3)\n",
    "print(f\"Mots similaires à 'parlement': {most_similar_example_2}\")\n",
    "\n",
    "\n",
    "most_similar_example_3 = model.wv.most_similar(\"entreprise\", topn=3)\n",
    "print(f\"Mots similaires à 'entreprise': {most_similar_example_3}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('tac_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "b1932ab1d169b4769d1550e799423b6477588e745f266d79d9004c136c81607e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
