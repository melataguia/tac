{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import string\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from pprint import pprint\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from unidecode import unidecode\n",
    "from timeout_decorator import timeout, TimeoutError\n",
    "from threading import Timer\n",
    "from yake import KeywordExtractor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Digit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/txt/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DECADE = '1900'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f for f in sorted(os.listdir(data_path)) if f\"_{DECADE[:-1]}\" in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choisir de la décennie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['KB_JB838_1900-01-02_01-00002.txt',\n",
       " 'KB_JB838_1900-01-07_01-00007.txt',\n",
       " 'KB_JB838_1900-01-08_01-00003.txt',\n",
       " 'KB_JB838_1900-01-18_01-00008.txt',\n",
       " 'KB_JB838_1900-01-19_01-00004.txt']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exemple de fichiers\n",
    "files[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [open(data_path + f, \"r\", encoding=\"utf-8\").read() for f in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'La bretelle, sur les origines de laquelle le;- •pinions sont contradictoires, n’en remonte pa,> moins à une honorable antiquité. De tous temps, bien que sous des formes diverses, elle a servi à maintenir la culotte et, plus tard, le pantalon. Sous Louis-Philippe, elle eut même son heure de gloire. On la vit aux culottes-à-pont de la garde nationale, et le roi lui-méme, dans telles estampes recherc'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exemple de textes\n",
    "texts[0][:400]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectoriser les documents à l'aide de TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une fonction de pré-traitement\n",
    "def preprocessing(text, stem=True):\n",
    "    \"\"\" Tokenize text and remove punctuation \"\"\"\n",
    "    text = text.translate(string.punctuation)\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instancier le modèle TF-IDF avec ses arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=preprocessing,\n",
    "    stop_words=stopwords.words('french'),\n",
    "    max_df=0.5,\n",
    "    min_df=0.1,\n",
    "    lowercase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construire la matrice de vecteurs à l'aide de la fonction `fit_transform`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Cours ULB\\Stic-2\\STIC-B545 - Traitement automatique de corpus\\Tp_traite_corpus2\\tac\\tac_venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Cours ULB\\Stic-2\\STIC-B545 - Traitement automatique de corpus\\Tp_traite_corpus2\\tac\\tp3\\tp3.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Cours%20ULB/Stic-2/STIC-B545%20-%20Traitement%20automatique%20de%20corpus/Tp_traite_corpus2/tac/tp3/tp3.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tfidf_vectors \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39;49mfit_transform(texts)\n",
      "File \u001b[1;32md:\\Cours ULB\\Stic-2\\STIC-B545 - Traitement automatique de corpus\\Tp_traite_corpus2\\tac\\tac_venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2139\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2132\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params()\n\u001b[0;32m   2133\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf \u001b[39m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2134\u001b[0m     norm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm,\n\u001b[0;32m   2135\u001b[0m     use_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_idf,\n\u001b[0;32m   2136\u001b[0m     smooth_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msmooth_idf,\n\u001b[0;32m   2137\u001b[0m     sublinear_tf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msublinear_tf,\n\u001b[0;32m   2138\u001b[0m )\n\u001b[1;32m-> 2139\u001b[0m X \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit_transform(raw_documents)\n\u001b[0;32m   2140\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf\u001b[39m.\u001b[39mfit(X)\n\u001b[0;32m   2141\u001b[0m \u001b[39m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2142\u001b[0m \u001b[39m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32md:\\Cours ULB\\Stic-2\\STIC-B545 - Traitement automatique de corpus\\Tp_traite_corpus2\\tac\\tac_venv\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\Cours ULB\\Stic-2\\STIC-B545 - Traitement automatique de corpus\\Tp_traite_corpus2\\tac\\tac_venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1389\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1381\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1382\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1385\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1386\u001b[0m             )\n\u001b[0;32m   1387\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1389\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   1391\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1392\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32md:\\Cours ULB\\Stic-2\\STIC-B545 - Traitement automatique de corpus\\Tp_traite_corpus2\\tac\\tac_venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1276\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1274\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[0;32m   1275\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[1;32m-> 1276\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[0;32m   1277\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1278\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32md:\\Cours ULB\\Stic-2\\STIC-B545 - Traitement automatique de corpus\\Tp_traite_corpus2\\tac\\tac_venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:112\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    110\u001b[0m     doc \u001b[39m=\u001b[39m preprocessor(doc)\n\u001b[0;32m    111\u001b[0m \u001b[39mif\u001b[39;00m tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 112\u001b[0m     doc \u001b[39m=\u001b[39m tokenizer(doc)\n\u001b[0;32m    113\u001b[0m \u001b[39mif\u001b[39;00m ngrams \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    114\u001b[0m     \u001b[39mif\u001b[39;00m stop_words \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;32md:\\Cours ULB\\Stic-2\\STIC-B545 - Traitement automatique de corpus\\Tp_traite_corpus2\\tac\\tp3\\tp3.ipynb Cell 17\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Cours%20ULB/Stic-2/STIC-B545%20-%20Traitement%20automatique%20de%20corpus/Tp_traite_corpus2/tac/tp3/tp3.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\" Tokenize text and remove punctuation \"\"\"\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Cours%20ULB/Stic-2/STIC-B545%20-%20Traitement%20automatique%20de%20corpus/Tp_traite_corpus2/tac/tp3/tp3.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m text \u001b[39m=\u001b[39m text\u001b[39m.\u001b[39mtranslate(string\u001b[39m.\u001b[39mpunctuation)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Cours%20ULB/Stic-2/STIC-B545%20-%20Traitement%20automatique%20de%20corpus/Tp_traite_corpus2/tac/tp3/tp3.ipynb#X22sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m tokens \u001b[39m=\u001b[39m word_tokenize(text)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Cours%20ULB/Stic-2/STIC-B545%20-%20Traitement%20automatique%20de%20corpus/Tp_traite_corpus2/tac/tp3/tp3.ipynb#X22sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mreturn\u001b[39;00m tokens\n",
      "File \u001b[1;32md:\\Cours ULB\\Stic-2\\STIC-B545 - Traitement automatique de corpus\\Tp_traite_corpus2\\tac\\tac_venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:130\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[39mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[39musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m:type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m--> 130\u001b[0m \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m     token \u001b[39mfor\u001b[39;49;00m sent \u001b[39min\u001b[39;49;00m sentences \u001b[39mfor\u001b[39;49;00m token \u001b[39min\u001b[39;49;00m _treebank_word_tokenizer\u001b[39m.\u001b[39;49mtokenize(sent)\n\u001b[0;32m    132\u001b[0m ]\n",
      "File \u001b[1;32md:\\Cours ULB\\Stic-2\\STIC-B545 - Traitement automatique de corpus\\Tp_traite_corpus2\\tac\\tac_venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:131\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[39mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[39musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m:type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    130\u001b[0m \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m--> 131\u001b[0m     token \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m _treebank_word_tokenizer\u001b[39m.\u001b[39;49mtokenize(sent)\n\u001b[0;32m    132\u001b[0m ]\n",
      "File \u001b[1;32md:\\Cours ULB\\Stic-2\\STIC-B545 - Traitement automatique de corpus\\Tp_traite_corpus2\\tac\\tac_venv\\Lib\\site-packages\\nltk\\tokenize\\destructive.py:161\u001b[0m, in \u001b[0;36mNLTKWordTokenizer.tokenize\u001b[1;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[0;32m    158\u001b[0m     text \u001b[39m=\u001b[39m regexp\u001b[39m.\u001b[39msub(substitution, text)\n\u001b[0;32m    160\u001b[0m \u001b[39mfor\u001b[39;00m regexp, substitution \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mPUNCTUATION:\n\u001b[1;32m--> 161\u001b[0m     text \u001b[39m=\u001b[39m regexp\u001b[39m.\u001b[39;49msub(substitution, text)\n\u001b[0;32m    163\u001b[0m \u001b[39m# Handles parentheses.\u001b[39;00m\n\u001b[0;32m    164\u001b[0m regexp, substitution \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mPARENS_BRACKETS\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\re\\__init__.py:322\u001b[0m, in \u001b[0;36m_subx.<locals>.filter\u001b[1;34m(match, template)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfilter\u001b[39m(match, template\u001b[39m=\u001b[39mtemplate):\n\u001b[1;32m--> 322\u001b[0m     \u001b[39mreturn\u001b[39;00m _parser\u001b[39m.\u001b[39;49mexpand_template(template, match)\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\re\\_parser.py:1090\u001b[0m, in \u001b[0;36mexpand_template\u001b[1;34m(template, match)\u001b[0m\n\u001b[0;32m   1087\u001b[0m         literals \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m s \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m s\u001b[39m.\u001b[39mencode(\u001b[39m'\u001b[39m\u001b[39mlatin-1\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m literals]\n\u001b[0;32m   1088\u001b[0m     \u001b[39mreturn\u001b[39;00m groups, literals\n\u001b[1;32m-> 1090\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexpand_template\u001b[39m(template, match):\n\u001b[0;32m   1091\u001b[0m     g \u001b[39m=\u001b[39m match\u001b[39m.\u001b[39mgroup\n\u001b[0;32m   1092\u001b[0m     empty \u001b[39m=\u001b[39m match\u001b[39m.\u001b[39mstring[:\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tfidf_vectors = vectorizer.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Détail de la matrice\n",
    "tfidf_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imprimer le vecteur tf-IDF du premier document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(\n",
    "    tfidf_vectors[0].toarray()[0],\n",
    "    index=vectorizer.get_feature_names_out()\n",
    "    ).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprendre les vecteurs et leurs \"distances\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine([1, 2, 3], [1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine([1, 2, 3], [1, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine([1, 2, 3], [2, 2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests sur nos documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_array = tfidf_vectors.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vecteur du document 0\n",
    "tfidf_array[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vecteur du document 1\n",
    "tfidf_array[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine(tfidf_array[0], tfidf_array[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appliquer un algorithme de clustering sur les vecteurs TF-IDF des documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour en savoir plus sur le KMeans clustering :\n",
    "- https://medium.com/dataseries/k-means-clustering-explained-visually-in-5-minutes-b900cc69d175"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définir un nombre de clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLUSTERS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instancier le modèle K-Means et ses arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_model = KMeans(n_clusters=N_CLUSTERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appliquer le clustering à l'aide de la fonction `fit_predict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = km_model.fit_predict(tfidf_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = collections.defaultdict(list)\n",
    "\n",
    "for idx, label in enumerate(clusters):\n",
    "    clustering[label].append(files[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(dict(clustering))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualiser le contenu des differents textes de chaque clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecture du  contenu des clusters avec une limite de temps\n",
    "def read_file_content(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "# gestion  du  délai d'exécution\n",
    "def timeout_handler():\n",
    "    raise TimeoutError(\"Function execution timed out\")\n",
    "\n",
    "# Exploration des données brutes de chaque cluster\n",
    "for cluster_label, files_in_cluster in clustering.items():\n",
    "    print(f\"Cluster {cluster_label} :\")\n",
    "    #pprint(files_in_cluster)\n",
    "    print(\"\\n\")\n",
    "\n",
    "   \n",
    "    content_list = []\n",
    "    for txt in files_in_cluster:\n",
    "        timer = Timer(5, timeout_handler)  \n",
    "        try:\n",
    "            timer.start()\n",
    "            content = read_file_content(os.path.join(data_path, txt))\n",
    "            content_list.append(content)\n",
    "        except TimeoutError:\n",
    "            print(f\"Reading {txt} took too long. Skipping.\")\n",
    "            continue\n",
    "        finally:\n",
    "            timer.cancel()\n",
    "\n",
    "    # Afficher le contenu de chaque texte dans le cluster\n",
    "    for txt in files_in_cluster:\n",
    "        try:\n",
    "            content = read_file_content(os.path.join(data_path, txt))\n",
    "            print(f\"Text in Cluster {cluster_label}:\")\n",
    "            print(content)\n",
    "            print(\"\\n\")\n",
    "        except TimeoutError:\n",
    "            print(f\"Reading {txt} took too long. Skipping.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stockage du contenu de chaque cluster dans une liste\n",
    "all_cluster_content = []\n",
    "\n",
    "# Exploration des données brutes de chaque cluster\n",
    "for cluster_label, files_in_cluster in clustering.items():\n",
    "   \n",
    "\n",
    "    \n",
    "    cluster_content = \"\"\n",
    "    for txt in files_in_cluster:\n",
    "        try:\n",
    "            content = read_file_content(os.path.join(data_path, txt))\n",
    "            cluster_content += content + \"\\n\"  \n",
    "        except TimeoutError:\n",
    "            print(f\"Reading {txt} took too long. Skipping.\")\n",
    "\n",
    "    # Ajouter le contenu du cluster au corpus global\n",
    "    all_cluster_content.append(cluster_content)\n",
    "\n",
    "# Afficher le contenu de chaque cluster\n",
    "for cluster_label, cluster_content in zip(clustering.keys(), all_cluster_content):\n",
    "    print(f\"Cluster {cluster_label} - All Content:\")\n",
    "    print(cluster_content)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction de keywords des differents clusters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keywords pour chaque clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kw_extractor = KeywordExtractor()\n",
    "\n",
    "all_cluster_keywords = []\n",
    "\n",
    "for cluster_label, files_in_cluster in clustering.items():\n",
    "  \n",
    "    cluster_content = \"\"\n",
    "    for txt in files_in_cluster:\n",
    "        try:\n",
    "            content = read_file_content(os.path.join(data_path, txt))\n",
    "            cluster_content += content + \"\\n\"  # Concaténer le contenu des textes avec un saut de ligne\n",
    "        except TimeoutError:\n",
    "            print(f\"Reading {txt} took too long. Skipping.\")\n",
    "\n",
    "    # Extraction des mots-clés du contenu du cluster\n",
    "    cluster_keywords = kw_extractor.extract_keywords(cluster_content)\n",
    "\n",
    "   \n",
    "    all_cluster_keywords.append(cluster_keywords)\n",
    "\n",
    "# Affichage des mots-clés de chaque cluster\n",
    "for cluster_label, cluster_keywords in zip(clustering.keys(), all_cluster_keywords):\n",
    "    print(f\"Cluster {cluster_label} - Keywords:\")\n",
    "    pprint(cluster_keywords)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_extractor = KeywordExtractor()\n",
    "\n",
    "for cluster_label, files_in_cluster in clustering.items():\n",
    "    \n",
    "\n",
    "    # Stocker le contenu de chaque texte dans le cluster\n",
    "    cluster_content = \"\"\n",
    "    for txt in files_in_cluster:\n",
    "        try:\n",
    "            content = read_file_content(os.path.join(data_path, txt))\n",
    "            cluster_content += content + \"\\n\"  # Concaténer le contenu des textes avec un saut de ligne\n",
    "        except TimeoutError:\n",
    "            print(f\"Reading {txt} took too long. Skipping.\")\n",
    "\n",
    "    # Extraire les mots-clés du contenu du cluster\n",
    "    cluster_keywords = kw_extractor.extract_keywords(cluster_content)\n",
    "\n",
    "    # Ajouter les mots-clés du cluster à la liste globale\n",
    "    all_cluster_keywords.extend(cluster_keywords)\n",
    "\n",
    "# Afficher les mots-clés de tous les clusters\n",
    "print(\"Keywords in All Clusters:\")\n",
    "kept_keywords = []\n",
    "for kw, score in all_cluster_keywords:\n",
    "    words = kw.split()\n",
    "    if len(words) == 2:\n",
    "        kept_keywords.append(kw)\n",
    "\n",
    "print(f\"Keywords: {', '.join(kept_keywords)}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualiser les clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse de nos clusters avec wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### definition de nos stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste de Stopwords enrichis (Idem que dans s1)\n",
    "sw = stopwords.words(\"french\")\n",
    "sw += [\"abord\", \"ailleurs\", \"ainsi\", \"alors\", \"après\", \"avant\", \"avoir\", \"bien\", \"bientôt\",\n",
    "    \"car\", \"ce\", \"ceci\", \"cela\", \"ces\", \"cet\", \"cette\", \"comme\", \"contre\", \"dans\",\n",
    "    \"depuis\", \"dire\", \"doit\", \"donc\", \"elle\", \"encore\", \"enfin\", \"ensuite\", \"entre\", \"être\",\n",
    "    \"fait\", \"faire\", \"faut\", \"hormis\", \"ici\", \"il\", \"ils\", \"jusqu'à\", \"les\", \"leur\", \"là\",\n",
    "    \"ma\", \"mais\", \"mes\", \"moins\", \"mon\", \"ne\", \"non\", \"nous\", \"on\", \"ou\", \"ou bien\", \"par\",\n",
    "    \"parce que\", \"parfois\", \"pas\", \"pendant\", \"peut\", \"plus\", \"plutôt\", \"pour\", \"puis\",\n",
    "    \"quand\", \"quant à\", \"sans\", \"sa\", \"se\", \"ses\", \"son\", \"sous\", \"tant pis\", \"tandis que\",\n",
    "    \"trois\", \"tôt ou tard\", \"toutefois\", \"toutes\", \"tu\", \"tous\", \"trois\", \"trois\", \"trois\",\n",
    "    \"voilà\", \"à\", \"très\"]\n",
    "sw = set(sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ecriture dans un fichier temporaire des differents clusters pour l'analyse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_path = '../data/tmp'\n",
    "if not os.path.exists(temp_path):\n",
    "    os.mkdir(temp_path)\n",
    "\n",
    "# Exploration des données brutes de chaque cluster\n",
    "for cluster_label, files_in_cluster in clustering.items():\n",
    "  \n",
    "\n",
    "    # Stocker le contenu de chaque texte dans le cluster\n",
    "    cluster_content = \"\"\n",
    "    for txt in files_in_cluster:\n",
    "        try:\n",
    "            content = read_file_content(os.path.join(data_path, txt))\n",
    "            cluster_content += content + \"\\n\"  # Concaténer le contenu des textes avec un saut de ligne\n",
    "        except TimeoutError:\n",
    "            print(f\"Reading {txt} took too long. Skipping.\")\n",
    "\n",
    "    # Écrire le contenu du cluster dans un fichier temporaire\n",
    "    with open(os.path.join(temp_path, f'cluster_{cluster_label}.txt'), 'w', encoding='utf-8') as f:\n",
    "        f.write(cluster_content)\n",
    "\n",
    "print(\"Content of all clusters has been written to temporary files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemin du fichier qui regroupera le contenu de tous les clusters\n",
    "output_path = os.path.join(temp_path, 'cluster_2.txt')\n",
    "\n",
    "# Imprimer le contenu du fichier et constater les \"déchets\"\n",
    "with open(output_path, 'r', encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def clean_text(content, folder=None, stop_words=None):\n",
    "    # Si le dossier n'est pas spécifié, utilisez le chemin par défaut\n",
    "    folder = folder or '.'\n",
    "\n",
    "    # Utilisez une liste de stop-words fournie ou une liste vide par défaut\n",
    "    sw = stop_words or []\n",
    "\n",
    "    if folder is None:\n",
    "        input_path = \"cluster_2.txt\"\n",
    "        output_path = \"all_clusters_clean.txt\"\n",
    "    else:\n",
    "        input_path = f\"{folder}/cluster_2.txt\"\n",
    "        output_path = f\"{folder}/all_clusters_clean.txt\"\n",
    "\n",
    "    # Créez le fichier de sortie pour écrire le texte nettoyé\n",
    "    with open(output_path, \"w\", encoding='utf-8') as output:\n",
    "        # Utilisez le contenu du cluster au lieu de lire depuis un fichier\n",
    "        text = cluster_content\n",
    "\n",
    "        # Tokenisez les mots en utilisant nltk\n",
    "        words = nltk.wordpunct_tokenize(text)\n",
    "\n",
    "        # Filtrez les mots en fonction des critères définis\n",
    "        kept = [w.upper() for w in words if len(w) > 2 and w.isalpha() and w.lower() not in sw]\n",
    "\n",
    "        # Joignez les mots filtrés pour former une chaîne\n",
    "        kept_string = \" \".join(kept)\n",
    "\n",
    "        # Écrivez la chaîne nettoyée dans le fichier de sortie\n",
    "        output.write(kept_string)\n",
    "\n",
    "    return f'Output has been written in {output_path}!'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text(content, folder=temp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(temp_path, f'all_clusters_clean.txt'), 'r', encoding='utf-8') as f:\n",
    "    after = f.read()\n",
    "\n",
    "print(after[:1000]) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Compter la fréquence des mots dans le texte nettoyé\n",
    "frequencies = Counter(after.split())\n",
    "\n",
    "# Afficher les 10 mots les plus fréquents\n",
    "print(frequencies.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Réduire les vecteurs à 2 dimensions à l'aide de l'algorithme PCA\n",
    "Cette étape est nécessaire afin de visualiser les documents dans un espace 2D\n",
    "\n",
    "https://fr.wikipedia.org/wiki/Analyse_en_composantes_principales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "reduced_vectors = pca.fit_transform(tfidf_vectors.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_vectors[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Générer le plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = reduced_vectors[:, 0]\n",
    "y_axis = reduced_vectors[:, 1]\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "scatter = plt.scatter(x_axis, y_axis, s=100, c=clusters)\n",
    "\n",
    "# Ajouter les centroïdes\n",
    "centroids = pca.transform(km_model.cluster_centers_)\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1],  marker = \"x\", s=100, linewidths = 2, color='black')\n",
    "\n",
    "# Ajouter la légende\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=set(clusters), title=\"Clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = reduced_vectors[:, 0]\n",
    "y_axis = reduced_vectors[:, 1]\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "scatter = plt.scatter(x_axis, y_axis, s=100, c=clusters)\n",
    "\n",
    "# Ajouter les centroïdes\n",
    "centroids = pca.transform(km_model.cluster_centers_)\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1],  marker = \"x\", s=100, linewidths = 2, color='black')\n",
    "\n",
    "# Ajouter la légende\n",
    "legend = plt.legend(handles=scatter.legend_elements()[0], labels=set(clusters), title=\"Clusters\")\n",
    "\n",
    "# Explorer chaque cluster\n",
    "for cluster_label in set(clusters):\n",
    "    cluster_points = reduced_vectors[clusters == cluster_label]\n",
    "    \n",
    "    # Afficher les points du cluster\n",
    "    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], s=50, label=f'Cluster {cluster_label}', alpha=0.5)\n",
    "\n",
    "plt.title('Clusters and Centroids')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster_label in set(clusters):\n",
    "    cluster_points = reduced_vectors[clusters == cluster_label]\n",
    "    \n",
    "    # Afficher les données brutes du cluster\n",
    "    print(f\"Cluster {cluster_label} - Raw Data:\")\n",
    "    print(cluster_points)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WORD_EMBEDDING "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création d'un objet qui *streame* les lignes d'un fichier pour économiser de la RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySentences(object):\n",
    "    \"\"\"Tokenize and Lemmatize sentences\"\"\"\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "\n",
    "    def __iter__(self):\n",
    "        for line in open(self.filename, encoding='utf-8', errors=\"backslashreplace\"):\n",
    "            yield [unidecode(w.lower()) for w in wordpunct_tokenize(line)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = f\"../data/sents.txt\"\n",
    "sentences = MySentences(infile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Détection des bigrams\n",
    "\n",
    "Article intéressant sur le sujet : https://towardsdatascience.com/word2vec-for-phrases-learning-embeddings-for-more-than-one-word-727b6cf723cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_phrases = Phrases(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'object `phrases` peut être vu comme un large dictionnaire d'expressions multi-mots associées à un score, le *PMI-like scoring*. Ce dictionnaire est construit par un apprentissage sur base d'exemples.\n",
    "Voir les références ci-dessous :\n",
    "- https://arxiv.org/abs/1310.4546\n",
    "- https://en.wikipedia.org/wiki/Pointwise_mutual_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(bigram_phrases.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il contient de nombreuses clés qui sont autant de termes observés dans le corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bigram_phrases.vocab.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prenons une clé au hasard :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_ = list(bigram_phrases.vocab.keys())[144]\n",
    "print(key_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le dictionnaire indique le score de cette coocurrence :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_phrases.vocab[key_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion des `Phrases` en objet `Phraser`\n",
    "\n",
    "`Phraser` est un alias pour `gensim.models.phrases.FrozenPhrases`, voir ici https://radimrehurek.com/gensim/models/phrases.html.\n",
    "\n",
    "Le `Phraser` est une version *light* du `Phrases`, plus optimale pour transformer les phrases en concaténant les bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_phraser = Phraser(phrases_model=bigram_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction des trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_phrases = Phrases(bigram_phraser[sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_phraser = Phraser(phrases_model=trigram_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création d'un corpus d'unigrams, bigrams, trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(trigram_phraser[bigram_phraser[sentences]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrainement d'un modèle Word2Vec sur ce corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = Word2Vec(\n",
    "    corpus, # On passe le corpus de ngrams que nous venons de créer\n",
    "    vector_size=32, # Le nombre de dimensions dans lesquelles le contexte des mots devra être réduit, aka. vector_size\n",
    "    window=5, # La taille du \"contexte\", ici 5 mots avant et après le mot observé\n",
    "    min_count=5, # On ignore les mots qui n'apparaissent pas au moins 5 fois dans le corpus\n",
    "    workers=4, # Permet de paralléliser l'entraînement du modèle en 4 threads\n",
    "    epochs=5 # Nombre d'itérations du réseau de neurones sur le jeu de données pour ajuster les paramètres avec la descente de gradient, aka. epochs.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Définir différentes valeurs pour window et min_count\n",
    "window_values = [3, 5, 7]\n",
    "min_count_values = [3, 5, 10]\n",
    "\n",
    "# Des essais-erreurs\n",
    "best_model = None\n",
    "best_score = float('-0.01')  \n",
    "\n",
    "for window_size in window_values:\n",
    "    for min_count_value in min_count_values:\n",
    "        print(f\"Entraînement avec window={window_size}, min_count={min_count_value}\")\n",
    "        \n",
    "        # Entraînement du modèle\n",
    "        model = Word2Vec(\n",
    "            corpus,\n",
    "            vector_size=32,\n",
    "            window=window_size,\n",
    "            min_count=min_count_value,\n",
    "            workers=4,\n",
    "            epochs=5\n",
    "        )\n",
    "\n",
    "        score = len(model.wv.key_to_index)\n",
    "\n",
    "        print(f\"Score obtenu : {score}\\n\")\n",
    "\n",
    "        # Comparaison avec le meilleur modèle précédent\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_model = model\n",
    "\n",
    "print(\"Entraînement terminé.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sauver le modèle dans un fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = f\"../data/newspapers.model\"\n",
    "model.save(outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorer le modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Charger le modèle en mémoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"../data/newspapers.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imprimer le vecteur d'un terme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv[\"ministre\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculer la similarité entre deux termes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity(\"ministre\", \"roi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chercher les mots les plus proches d'un terme donné"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(\"ministre\", topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faire des recherches complexes à travers l'espace vectoriel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.most_similar(positive=['paris', 'londres'], negative=['belgique']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution de 06 exemples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "similarity_example_1 = model.wv.similarity('paris', 'france')\n",
    "print(f\"Similarité entre 'paris' et 'france': {similarity_example_1}\")\n",
    "\n",
    "\n",
    "similarity_example_2 = model.wv.similarity('londres', 'royaume_uni')\n",
    "print(f\"Similarité entre 'londres' et 'royaume_uni': {similarity_example_2}\")\n",
    "\n",
    "\n",
    "similarity_example_3 = model.wv.similarity('belgique', 'france')\n",
    "print(f\"Similarité entre 'belgique' et 'france': {similarity_example_3}\")\n",
    "\n",
    "\n",
    "most_similar_example_1 = model.wv.most_similar(\"ministre\", topn=3)\n",
    "print(f\"Mots similaires à 'ministre': {most_similar_example_1}\")\n",
    "\n",
    "\n",
    "most_similar_example_2 = model.wv.most_similar(\"parlement\", topn=3)\n",
    "print(f\"Mots similaires à 'parlement': {most_similar_example_2}\")\n",
    "\n",
    "\n",
    "most_similar_example_3 = model.wv.most_similar(\"entreprise\", topn=3)\n",
    "print(f\"Mots similaires à 'entreprise': {most_similar_example_3}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('tac_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "b1932ab1d169b4769d1550e799423b6477588e745f266d79d9004c136c81607e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
