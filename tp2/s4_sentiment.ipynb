{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis \n",
    "\n",
    "## 1. Textblob-FR\n",
    "\n",
    "Documentation: https://textblob.readthedocs.io/en/dev/\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from textblob import Blobber\n",
    "from textblob_fr import PatternTagger, PatternAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création d'une fonction `get_sentiment`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = Blobber(pos_tagger=PatternTagger(), analyzer=PatternAnalyzer())\n",
    "\n",
    "def get_sentiment(input_text):\n",
    "    blob = tb(input_text)\n",
    "    polarity, subjectivity = blob.sentiment\n",
    "    polarity_perc = f\"{100*abs(polarity):.0f}\"\n",
    "    subjectivity_perc = f\"{100*subjectivity:.0f}\"\n",
    "    if polarity > 0:\n",
    "        polarity_str = f\"{polarity_perc}% positive\"\n",
    "    elif polarity < 0:\n",
    "        polarity_str = f\"{polarity_perc}% negative\"\n",
    "    else:\n",
    "        polarity_str = \"neutral\"\n",
    "    if subjectivity > 0:\n",
    "        subjectivity_str = f\"{subjectivity}% subjective\"\n",
    "    else:\n",
    "        subjectivity_str = \"perfectly objective\"\n",
    "    print(f\"This text is {polarity_str} and {subjectivity_str}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyser le sentiment d'une phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choisir une année\n",
    "year = 1900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lister les fichiers de cette année\n",
    "data_path = '../data'\n",
    "txt_path = '../data/txt'\n",
    "txts = [f for f in os.listdir(txt_path) if os.path.isfile(os.path.join(txt_path, f)) and str(year) in f]\n",
    "len(txts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stocker le contenu de ces fichiers dans une liste\n",
    "content_list = []\n",
    "for txt in txts:\n",
    "    with open(os.path.join(txt_path, txt), 'r', encoding='utf-8') as f:\n",
    "        content_list.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compter le nombre d'éléments (=fichiers) dans la liste\n",
    "len(content_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste pour stocker les phrases\n",
    "selected_phrases = []\n",
    "\n",
    "# Compteur pour les documents et les phrases\n",
    "document_count = 0\n",
    "phrase_count = 0\n",
    "\n",
    "# Longueur minimale requise pour une phrase\n",
    "min_phrase_length = 30\n",
    "\n",
    "# Parcour des 10 premiers fichiers de la liste\n",
    "for document in content_list[:10]:\n",
    "    # Division du fichier en trois parties égales\n",
    "    part_length = len(document) // 3\n",
    "    \n",
    "    # Phrases au début\n",
    "    start_phrases = []\n",
    "    current_phrase = \"\"\n",
    "    for char in document[:part_length]:\n",
    "        current_phrase += char\n",
    "        if char in ['.', '!', '?']:\n",
    "            if len(current_phrase) >= min_phrase_length:\n",
    "                start_phrases.append(current_phrase)\n",
    "                current_phrase = \"\"\n",
    "    \n",
    "    # Phrases au milieu\n",
    "    middle_phrases = []\n",
    "    current_phrase = \"\"\n",
    "    for char in document[part_length:2*part_length]:\n",
    "        current_phrase += char\n",
    "        if char in ['.', '!', '?']:\n",
    "            if len(current_phrase) >= min_phrase_length:\n",
    "                middle_phrases.append(current_phrase)\n",
    "                current_phrase = \"\"\n",
    "    \n",
    "    # Phrases à la fin\n",
    "    end_phrases = []\n",
    "    current_phrase = \"\"\n",
    "    for char in document[2*part_length:]:\n",
    "        current_phrase += char\n",
    "        if char in ['.', '!', '?']:\n",
    "            if len(current_phrase) >= min_phrase_length:\n",
    "                end_phrases.append(current_phrase)\n",
    "                current_phrase = \"\"\n",
    "    \n",
    "    # Sélection de 3 phrases au début, 3 au milieu et 4 à la fin\n",
    "    selected_phrases.extend(start_phrases[:3])\n",
    "    selected_phrases.extend(middle_phrases[:3])\n",
    "    selected_phrases.extend(end_phrases[:4])\n",
    "    \n",
    "    # Mise à jour des compteurs\n",
    "    document_count += 1\n",
    "    phrase_count += len(start_phrases[:3]) + len(middle_phrases[:3]) + len(end_phrases[:4])\n",
    "    \n",
    "    if phrase_count >= 10:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " selected_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\\n\".join(selected_phrases[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_sentiment(\"Les journaux publient la dépêche suivante du camp de Frère, 31 : Les Boers ont établi un nouveau camp formé do 63 wagons.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_sentiment(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Utilisation de transformers\n",
    "\n",
    "Documentation: https://github.com/TheophileBlard/french-sentiment-analysis-with-bert\n",
    "\n",
    "**!!** Si le code ne tourne pas sur votre machine, vous pouvez le tester directement sur Google Colab en utilisant [ce lien](https://colab.research.google.com/github/TheophileBlard/french-sentiment-analysis-with-bert/blob/master/colab/french_sentiment_analysis_with_bert.ipynb) **!!**\n",
    "\n",
    "Le modèle peut également être testé en ligne sur [HuggingFace](https://huggingface.co/tblard/tf-allocine)\n",
    "\n",
    "### Installation des librairies et imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.15.0-cp311-cp311-win_amd64.whl.metadata (3.6 kB)\n",
      "Collecting tensorflow-intel==2.15.0 (from tensorflow)\n",
      "  Downloading tensorflow_intel-2.15.0-cp311-cp311-win_amd64.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in d:\\cours ulb\\stic-2\\stic-b545 - traitement automatique de corpus\\tp_traite_corpus2\\tac\\tac_venv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in d:\\cours ulb\\stic-2\\stic-b545 - traitement automatique de corpus\\tp_traite_corpus2\\tac\\tac_venv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in d:\\cours ulb\\stic-2\\stic-b545 - traitement automatique de corpus\\tp_traite_corpus2\\tac\\tac_venv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in d:\\cours ulb\\stic-2\\stic-b545 - traitement automatique de corpus\\tp_traite_corpus2\\tac\\tac_venv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in d:\\cours ulb\\stic-2\\stic-b545 - traitement automatique de corpus\\tp_traite_corpus2\\tac\\tac_venv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in d:\\cours ulb\\stic-2\\stic-b545 - traitement automatique de corpus\\tp_traite_corpus2\\tac\\tac_venv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in d:\\cours ulb\\stic-2\\stic-b545 - traitement automatique de corpus\\tp_traite_corpus2\\tac\\tac_venv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in d:\\cours ulb\\stic-2\\stic-b545 - traitement automatique de corpus\\tp_traite_corpus2\\tac\\tac_venv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in d:\\cours ulb\\stic-2\\stic-b545 - traitement automatique de corpus\\tp_traite_corpus2\\tac\\tac_venv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.26.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in d:\\cours ulb\\stic-2\\stic-b545 - traitement automatique de corpus\\tp_traite_corpus2\\tac\\tac_venv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in d:\\cours ulb\\stic-2\\stic-b545 - traitement automatique de corpus\\tp_traite_corpus2\\tac\\tac_venv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in d:\\cours ulb\\stic-2\\stic-b545 - traitement automatique de corpus\\tp_traite_corpus2\\tac\\tac_venv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.24.4)\n",
      "Requirement already satisfied: setuptools in d:\\cours ulb\\stic-2\\stic-b545 - traitement automatique de corpus\\tp_traite_corpus2\\tac\\tac_venv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in d:\\cours ulb\\stic-2\\stic-b545 - traitement automatique de corpus\\tp_traite_corpus2\\tac\\tac_venv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.12.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in d:\\cours ulb\\stic-2\\stic-b545 - traitement automatique de corpus\\tp_traite_corpus2\\tac\\tac_venv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in d:\\cours ulb\\stic-2\\stic-b545 - traitement automatique de corpus\\tp_traite_corpus2\\tac\\tac_venv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.8.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in d:\\cours ulb\\stic-2\\stic-b545 - traitement automatique de corpus\\tp_traite_corpus2\\tac\\tac_venv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in d:\\cours ulb\\stic-2\\stic-b545 - traitement automatique de corpus\\tp_traite_corpus2\\tac\\tac_venv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in d:\\cours ulb\\stic-2\\stic-b545 - traitement automatique de corpus\\tp_traite_corpus2\\tac\\tac_venv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.59.0)\n",
      "Collecting tensorboard<2.16,>=2.15 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Downloading tensorboard-2.15.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting keras<2.16,>=2.15.0 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in d:\\cours ulb\\stic-2\\stic-b545 - traitement automatique de corpus\\tp_traite_corpus2\\tac\\tac_venv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.15.0->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in d:\\cours ulb\\stic-2\\stic-b545 - traitement automatique de corpus\\tp_traite_corpus2\\tac\\tac_venv\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.23.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in d:\\cours ulb\\stic-2\\stic-b545 - traitement automatique de corpus\\tp_traite_corpus2\\tac\\tac_venv\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in d:\\cours ulb\\stic-2\\stic-b545 - traitement automatique de corpus\\tp_traite_corpus2\\tac\\tac_venv\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.5)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Downloading protobuf-4.23.4-cp310-abi3-win_amd64.whl.metadata (540 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in d:\\cours ulb\\stic-2\\stic-b545 - traitement automatique de corpus\\tp_traite_corpus2\\tac\\tac_venv\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in d:\\cours ulb\\stic-2\\stic-b545 - traitement automatique de corpus\\tp_traite_corpus2\\tac\\tac_venv\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in d:\\cours ulb\\stic-2\\stic-b545 - traitement automatique de corpus\\tp_traite_corpus2\\tac\\tac_venv\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in d:\\cours ulb\\stic-2\\stic-b545 - traitement automatique de corpus\\tp_traite_corpus2\\tac\\tac_venv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in d:\\cours ulb\\stic-2\\stic-b545 - traitement automatique de corpus\\tp_traite_corpus2\\tac\\tac_venv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in d:\\cours ulb\\stic-2\\stic-b545 - traitement automatique de corpus\\tp_traite_corpus2\\tac\\tac_venv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in d:\\cours ulb\\stic-2\\stic-b545 - traitement automatique de corpus\\tp_traite_corpus2\\tac\\tac_venv\\lib\\site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\cours ulb\\stic-2\\stic-b545 - traitement automatique de corpus\\tp_traite_corpus2\\tac\\tac_venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\cours ulb\\stic-2\\stic-b545 - traitement automatique de corpus\\tp_traite_corpus2\\tac\\tac_venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\cours ulb\\stic-2\\stic-b545 - traitement automatique de corpus\\tp_traite_corpus2\\tac\\tac_venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\cours ulb\\stic-2\\stic-b545 - traitement automatique de corpus\\tp_traite_corpus2\\tac\\tac_venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in d:\\cours ulb\\stic-2\\stic-b545 - traitement automatique de corpus\\tp_traite_corpus2\\tac\\tac_venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in d:\\cours ulb\\stic-2\\stic-b545 - traitement automatique de corpus\\tp_traite_corpus2\\tac\\tac_venv\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in d:\\cours ulb\\stic-2\\stic-b545 - traitement automatique de corpus\\tp_traite_corpus2\\tac\\tac_venv\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.2.2)\n",
      "Downloading tensorflow-2.15.0-cp311-cp311-win_amd64.whl (2.1 kB)\n",
      "Downloading tensorflow_intel-2.15.0-cp311-cp311-win_amd64.whl (300.9 MB)\n",
      "   -------------------------------------    278.9/300.9 MB 4.6 MB/s eta 0:00:05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Cours ULB\\Stic-2\\STIC-B545 - Traitement automatique de corpus\\Tp_traite_corpus2\\tac\\tac_venv\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 438, in _error_catcher\n",
      "    yield\n",
      "  File \"D:\\Cours ULB\\Stic-2\\STIC-B545 - Traitement automatique de corpus\\Tp_traite_corpus2\\tac\\tac_venv\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 561, in read\n",
      "    data = self._fp_read(amt) if not fp_closed else b\"\"\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\Cours ULB\\Stic-2\\STIC-B545 - Traitement automatique de corpus\\Tp_traite_corpus2\\tac\\tac_venv\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 527, in _fp_read\n",
      "    return self._fp.read(amt) if amt is not None else self._fp.read()\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\Cours ULB\\Stic-2\\STIC-B545 - Traitement automatique de corpus\\Tp_traite_corpus2\\tac\\tac_venv\\Lib\\site-packages\\pip\\_vendor\\cachecontrol\\filewrapper.py\", line 102, in read\n",
      "    self.__buf.write(data)\n",
      "  File \"C:\\Python311\\Lib\\tempfile.py\", line 483, in func_wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "OSError: [Errno 28] No space left on device\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Cours ULB\\Stic-2\\STIC-B545 - Traitement automatique de corpus\\Tp_traite_corpus2\\tac\\tac_venv\\Lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 180, in exc_logging_wrapper\n",
      "    status = run_func(*args)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"D:\\Cours ULB\\Stic-2\\STIC-B545 - Traitement automatique de corpus\\Tp_traite_corpus2\\tac\\tac_venv\\Lib\\site-packages\\pip\\_internal\\cli\\req_command.py\", line 245, in wrapper\n",
      "    return func(self, options, args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\Cours ULB\\Stic-2\\STIC-B545 - Traitement automatique de corpus\\Tp_traite_corpus2\\tac\\tac_venv\\Lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 377, in run\n",
      "    requirement_set = resolver.resolve(\n",
      "                      ^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\Cours ULB\\Stic-2\\STIC-B545 - Traitement automatique de corpus\\Tp_traite_corpus2\\tac\\tac_venv\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\resolver.py\", line 179, in resolve\n",
      "    self.factory.preparer.prepare_linked_requirements_more(reqs)\n",
      "  File \"D:\\Cours ULB\\Stic-2\\STIC-B545 - Traitement automatique de corpus\\Tp_traite_corpus2\\tac\\tac_venv\\Lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 552, in prepare_linked_requirements_more\n",
      "    self._complete_partial_requirements(\n",
      "  File \"D:\\Cours ULB\\Stic-2\\STIC-B545 - Traitement automatique de corpus\\Tp_traite_corpus2\\tac\\tac_venv\\Lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 467, in _complete_partial_requirements\n",
      "    for link, (filepath, _) in batch_download:\n",
      "  File \"D:\\Cours ULB\\Stic-2\\STIC-B545 - Traitement automatique de corpus\\Tp_traite_corpus2\\tac\\tac_venv\\Lib\\site-packages\\pip\\_internal\\network\\download.py\", line 183, in __call__\n",
      "    for chunk in chunks:\n",
      "  File \"D:\\Cours ULB\\Stic-2\\STIC-B545 - Traitement automatique de corpus\\Tp_traite_corpus2\\tac\\tac_venv\\Lib\\site-packages\\pip\\_internal\\cli\\progress_bars.py\", line 53, in _rich_progress_bar\n",
      "    for chunk in iterable:\n",
      "  File \"D:\\Cours ULB\\Stic-2\\STIC-B545 - Traitement automatique de corpus\\Tp_traite_corpus2\\tac\\tac_venv\\Lib\\site-packages\\pip\\_internal\\network\\utils.py\", line 63, in response_chunks\n",
      "    for chunk in response.raw.stream(\n",
      "  File \"D:\\Cours ULB\\Stic-2\\STIC-B545 - Traitement automatique de corpus\\Tp_traite_corpus2\\tac\\tac_venv\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 622, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\Cours ULB\\Stic-2\\STIC-B545 - Traitement automatique de corpus\\Tp_traite_corpus2\\tac\\tac_venv\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 560, in read\n",
      "    with self._error_catcher():\n",
      "  File \"C:\\Python311\\Lib\\contextlib.py\", line 155, in __exit__\n",
      "    self.gen.throw(typ, value, traceback)\n",
      "  File \"D:\\Cours ULB\\Stic-2\\STIC-B545 - Traitement automatique de corpus\\Tp_traite_corpus2\\tac\\tac_venv\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 455, in _error_catcher\n",
      "    raise ProtocolError(\"Connection broken: %r\" % e, e)\n",
      "pip._vendor.urllib3.exceptions.ProtocolError: (\"Connection broken: OSError(28, 'No space left on device')\", OSError(28, 'No space left on device'))\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n",
    "!pip install sentencepiece\n",
    "!pip install transformers\n",
    "!pip install spacy-transformer\n",
    "\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"tblard/tf-allocine\", use_pt=True)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"tblard/tf-allocine\")\n",
    "\n",
    "sentiment_analyser = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyser le sentiment d'une phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyser(\"Ce journal est vraiment super intéressant.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyser(\"Cette phrase est négative et je ne suis pas content !\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('tac_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "b1932ab1d169b4769d1550e799423b6477588e745f266d79d9004c136c81607e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
