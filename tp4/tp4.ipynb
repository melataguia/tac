{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TP4(Projet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import yake\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from IPython.display import Image\n",
    "import sys\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import spacy\n",
    "from textblob import Blobber\n",
    "from textblob_fr import PatternTagger, PatternAnalyzer\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from unidecode import unidecode\n",
    "import collections\n",
    "import string\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "from pprint import pprint\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantier l'extracteur de mots clés\n",
    "kw_extractor = yake.KeywordExtractor(lan=\"fr\", top=50)\n",
    "kw_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste de Stopwords enrichis (Idem que dans s1)\n",
    "sw = stopwords.words(\"french\")\n",
    "sw += [\"abord\",\"deux\",\"tout\", \"ailleurs\", \"ainsi\", \"alors\", \"après\", \"avant\", \"avoir\", \"bien\", \"bientôt\",\n",
    "    \"car\", \"ce\", \"ceci\", \"cela\", \"ces\", \"cet\", \"cette\", \"comme\", \"contre\", \"dans\",\n",
    "    \"depuis\", \"dire\", \"doit\", \"donc\", \"elle\", \"encore\", \"enfin\", \"ensuite\", \"entre\", \"être\",\n",
    "    \"fait\", \"faire\", \"faut\", \"hormis\", \"ici\", \"il\", \"ils\", \"jusqu'à\", \"les\", \"leur\", \"là\",\n",
    "    \"ma\", \"mais\", \"mes\", \"moins\", \"mon\", \"ne\", \"non\", \"nous\", \"on\", \"ou\", \"ou bien\", \"par\",\n",
    "    \"parce que\", \"parfois\", \"pas\", \"pendant\", \"peut\", \"plus\", \"plutôt\", \"pour\", \"puis\",\n",
    "    \"quand\", \"quant à\", \"sans\", \"sa\", \"se\", \"ses\", \"son\", \"sous\", \"tant pis\", \"tandis que\",\n",
    "    \"trois\", \"tôt ou tard\", \"toutefois\", \"toutes\", \"tu\", \"tous\", \"trois\", \"trois\", \"trois\",\n",
    "    \"voilà\", \"à\", \"très\"]\n",
    "sw = set(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/txttp4/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Chemin vers les differents fichiers de notre travail "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f for f in sorted(os.listdir(data_path)) if f.endswith('.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taille des fichiers\n",
    "files\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de fichiers\n",
    "files[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stocker le contenu de ces fichiers dans une liste en utilisant la compréhension de liste\n",
    "texts = [open(data_path + f, \"r\", encoding=\"utf-8\").read() for f in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for text in texts:\n",
    "    #for char in text:\n",
    "       # print(char, end='', flush=True)\n",
    "        #time.sleep(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ecrire tout le contenu dans un fichier temporaire\n",
    "temp_path = '../data/tmp'\n",
    "if not os.path.exists(temp_path):\n",
    "    os.mkdir(temp_path)\n",
    "with open(os.path.join(temp_path, f'texts.txt'), 'w', encoding='utf-8') as f:\n",
    "    f.write(' '.join(texts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimer le contenu du fichier et constater les \"déchets\"\n",
    "with open(os.path.join(temp_path, f'texts.txt'), 'r', encoding='utf-8') as f:\n",
    "    before = f.read()\n",
    "\n",
    "before[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(folder=None):\n",
    "    if folder is None:\n",
    "        input_path = f\"texts.txt\"\n",
    "        output_path = f\"texts_clean.txt\"\n",
    "    else:\n",
    "        input_path = f\"{folder}/texts.txt\"\n",
    "        output_path = f\"{folder}/texts_clean.txt\"\n",
    "    output = open(output_path, \"w\", encoding='utf-8')\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        words = nltk.wordpunct_tokenize(text)\n",
    "        kept = [w.upper() for w in words if len(w) > 2 and w.isalpha() and w.lower() not in sw]\n",
    "        kept_string = \" \".join(kept)\n",
    "        output.write(kept_string)\n",
    "    return f'Output has been written in {output_path}!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text( folder=temp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier le résultat\n",
    "with open(os.path.join(temp_path, f'texts_clean.txt'), 'r', encoding='utf-8') as f:\n",
    "    after = f.read()\n",
    "\n",
    "after[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Extraire uniquement les phrases \"pertinentes\" dans les documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = [\"volley\", \"volleyball\",\"volley-ball\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une regex afin de trouver les mots de la liste query dans le texte\n",
    "regex = re.compile(f\"\\\\b({'|'.join(query)})\\\\b\", re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le répertoire qui contient vos fichiers txt exportés de Camille\n",
    "indir = \"../data/tmp/\"\n",
    "# Le répertoire qui contiendra les fichiers txt nettoyés\n",
    "outdir = \"../data/tmp/regex/\"\n",
    "\n",
    "if not os.path.exists(outdir):\n",
    "    os.mkdir(outdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(indir)[:10]:\n",
    "    if file.endswith(\"texts_clean.txt\"):\n",
    "        relevant_sentences = []\n",
    "        f_in = open(os.path.join(indir, file), encoding=\"utf-8\")\n",
    "        text = f_in.read()\n",
    "        for sentence in sent_tokenize(text):\n",
    "            if regex.search(sentence):\n",
    "                relevant_sentences.append(sentence)\n",
    "        f_in.close()\n",
    "        f_out = open(os.path.join(outdir, file), \"w\", encoding=\"utf-8\")\n",
    "        f_out.write(\"\\n\\n\".join(relevant_sentences))\n",
    "        f_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier le résultat\n",
    "with open(os.path.join(outdir, f'texts_clean.txt'), 'r', encoding='utf-8') as f:\n",
    "    after = f.read()\n",
    "\n",
    "after[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Afficher les termes les plus fréquents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies = Counter(after.split())\n",
    "print(frequencies.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nuage de mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud = WordCloud(width=2000, height=1000, background_color='white').generate_from_frequencies(frequencies)\n",
    "cloud.to_file(os.path.join(temp_path, f\"texts.png\"))\n",
    "Image(filename=os.path.join(temp_path, f\"texts.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_file = files\n",
    "this_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation de liste pour stocker les textes\n",
    "texts_n = []\n",
    "\n",
    "# Parcourir la liste des noms de fichiers\n",
    "for this_file in this_file:\n",
    "    file_path = os.path.join(data_path, this_file)\n",
    "    \n",
    "    # Vérifier si le fichier existe\n",
    "    if os.path.exists(file_path):\n",
    "        # Lecture du contenu du fichier et stockage dans la liste\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            file_text = file.read()\n",
    "            texts_n.append(file_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraire les mots clés de ce texte\n",
    "keywords = kw_extractor.extract_keywords(file_text)\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ne garder que les bigrammes\n",
    "kept = []\n",
    "for kw, score in keywords:\n",
    "    words = kw.split()\n",
    "    if len(words) == 2:\n",
    "        kept.append(kw)\n",
    "kept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolution de thematiques de volleyball au fil des annees "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deplacement des fichiers dans les dossiers correspondant a chaque annee "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# important de faire une copie du repertoire principal de fichier avant  de deplacer les fichiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Chemin vers le répertoire contenant les fichiers\n",
    "data_path = \"../data/txttp4/\"\n",
    "\n",
    "# Créer un répertoire pour chaque année entre 1830 et 1970\n",
    "for year in range(1830, 1971):\n",
    "    year_folder = f\"../data/txttp4/{year}\"\n",
    "    os.makedirs(year_folder, exist_ok=True)\n",
    "\n",
    "# Liste des fichiers dans le répertoire principal\n",
    "files = os.listdir(data_path)\n",
    "\n",
    "# Déplacer les fichiers vers les sous-répertoires correspondants\n",
    "for file in files:\n",
    "    # Extraire l'année du nom de fichier\n",
    "    try:\n",
    "        year = int(file.split('_')[2].split('-')[0])\n",
    "    except (ValueError, IndexError):\n",
    "        # Ignorer les fichiers qui ne suivent pas le format attendu\n",
    "        continue\n",
    "\n",
    "    # Afficher le chemin du fichier et le chemin de destination\n",
    "    source_path = os.path.join(data_path, file)\n",
    "    destination_path = os.path.join(data_path, str(year), file)\n",
    "    print(f\"Moving {file} from {source_path} to {destination_path}\")\n",
    "\n",
    "    try:\n",
    "        # Déplacer le fichier vers le sous-répertoire correspondant à l'année\n",
    "        shutil.move(source_path, destination_path)\n",
    "        print(f\"Successfully moved {file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error moving {file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Chemin vers le répertoire principal\n",
    "data_path = \"../data/txttp4/\"\n",
    "\n",
    "# Liste des années\n",
    "years = list(range(1830, 1971))\n",
    "\n",
    "# Nombre de fichiers par année\n",
    "file_counts = []\n",
    "\n",
    "# Parcourir les années et compter le nombre de fichiers dans chaque dossier\n",
    "for year in years:\n",
    "    year_folder = os.path.join(data_path, str(year))\n",
    "    if os.path.exists(year_folder):\n",
    "        files_in_year = len(os.listdir(year_folder))\n",
    "        file_counts.append(files_in_year)\n",
    "    else:\n",
    "        file_counts.append(0)\n",
    "\n",
    "# Tracer le graphique\n",
    "plt.plot(years, file_counts, marker='o')\n",
    "plt.title('Nombre de fichiers par année')\n",
    "plt.xlabel('Année')\n",
    "plt.ylabel('Nombre de fichiers')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etude des corpus en fonctions des intervalles d'annees ou annees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Chemin vers le répertoire principal\n",
    "data_path = \"../data/txttp4/\"\n",
    "\n",
    "# Intervalles d'années que vous souhaitez extraire\n",
    "start_year = 1941\n",
    "end_year = 1955  \n",
    "\n",
    "# Liste pour stocker les noms de fichiers\n",
    "N_files = []\n",
    "\n",
    "# Parcourir les années dans l'intervalle spécifié\n",
    "for year in range(start_year, end_year + 1):  # +1 pour inclure également end_year\n",
    "    year_folder = os.path.join(data_path, str(year))\n",
    "    \n",
    "    # Ajouter les fichiers se terminant par '.txt' dans la liste\n",
    "    N_files.extend([os.path.join(year_folder, f) for f in sorted(os.listdir(year_folder)) if f.endswith('.txt')])\n",
    "\n",
    "# Afficher les noms de fichiers\n",
    "print(N_files)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre de fichiers \n",
    "len(N_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stocker le contenu de ces fichiers dans une liste en utilisant la compréhension de liste\n",
    "N_texts = [open(f, \"r\", encoding=\"utf-8\").read() for f in N_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ecrire tout le contenu dans un fichier temporaire\n",
    "temp_path = '../data/tmp'\n",
    "if not os.path.exists(temp_path):\n",
    "    os.mkdir(temp_path)\n",
    "with open(os.path.join(temp_path, f'N_texts.txt'), 'w', encoding='utf-8') as f:\n",
    "    f.write(' '.join(N_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimer le contenu du fichier et constater les \"déchets\"\n",
    "with open(os.path.join(temp_path, f'N_texts.txt'), 'r', encoding='utf-8') as f:\n",
    "    before = f.read()\n",
    "\n",
    "before[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(folder=None):\n",
    "    if folder is None:\n",
    "        input_path = f\"N_texts.txt\"\n",
    "        output_path = f\"N_texts_clean.txt\"\n",
    "    else:\n",
    "        input_path = f\"{folder}/N_texts.txt\"\n",
    "        output_path = f\"{folder}/N_texts_clean.txt\"\n",
    "    output = open(output_path, \"w\", encoding='utf-8')\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        words = nltk.wordpunct_tokenize(text)\n",
    "        kept = [w.upper() for w in words if len(w) > 2 and w.isalpha() and w.lower() not in sw]\n",
    "        kept_string = \" \".join(kept)\n",
    "        output.write(kept_string)\n",
    "    return f'Output has been written in {output_path}!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text( folder=temp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier le résultat\n",
    "with open(os.path.join(temp_path, f'texts_clean.txt'), 'r', encoding='utf-8') as f:\n",
    "    after = f.read()\n",
    "\n",
    "after[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = [\"volley\", \"volleyball\",\"volley-ball\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une regex afin de trouver les mots de la liste query dans le texte\n",
    "regex = re.compile(f\"\\\\b({'|'.join(query)})\\\\b\", re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le répertoire qui contient vos fichiers txt exportés de Camille\n",
    "indir = \"../data/tmp/\"\n",
    "# Le répertoire qui contiendra les fichiers txt nettoyés\n",
    "outdir = \"../data/tmp/regex/\"\n",
    "\n",
    "if not os.path.exists(outdir):\n",
    "    os.mkdir(outdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(indir)[:10]:\n",
    "    if file.endswith(\"N_texts_clean.txt\"):\n",
    "        relevant_sentences = []\n",
    "        f_in = open(os.path.join(indir, file), encoding=\"utf-8\")\n",
    "        text = f_in.read()\n",
    "        for sentence in sent_tokenize(text):\n",
    "            if regex.search(sentence):\n",
    "                relevant_sentences.append(sentence)\n",
    "        f_in.close()\n",
    "        f_out = open(os.path.join(outdir, file), \"w\", encoding=\"utf-8\")\n",
    "        f_out.write(\"\\n\\n\".join(relevant_sentences))\n",
    "        f_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier le résultat\n",
    "with open(os.path.join(outdir, f'N_texts_clean.txt'), 'r', encoding='utf-8') as f:\n",
    "    after = f.read()\n",
    "\n",
    "after[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies = Counter(after.split())\n",
    "print(frequencies.most_common(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud = WordCloud(width=2000, height=1000, background_color='white').generate_from_frequencies(frequencies)\n",
    "cloud.to_file(os.path.join(temp_path, f\"N_texts.png\"))\n",
    "Image(filename=os.path.join(temp_path, f\"N_texts.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les mots clés de mon sous corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rake_nltk import Rake\n",
    "\n",
    "# Charger le contenu du fichier Texts_clean\n",
    "with open(\"../data/tmp/regex/N_texts_clean.txt\", 'r', encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Initialiser l'extracteur de mots-clés Rake avec des paramètres personnalisés\n",
    "kw_extractor = Rake(min_length=2, max_length=5)  # Vous pouvez ajuster ces valeurs selon vos besoins\n",
    "\n",
    "# Extraire les mots-clés du texte\n",
    "kw_extractor.extract_keywords_from_text(text)\n",
    "\n",
    "# Récupérer les mots-clés\n",
    "keywords = kw_extractor.get_ranked_phrases()\n",
    "\n",
    "# Filtrer les mots-clés pour ne conserver que ceux de deux mots\n",
    "kept = [kw for kw in keywords if len(kw.split()) == 2]\n",
    "\n",
    "# Afficher les mots-clés conservés\n",
    "print(f\"Mots-clés conservés : {', '.join(kept)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les noms des personnes et des endroits pour voir comment les acteurs principaux du volleyball en Belgique ont évolué"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import defaultdict\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Initialiser le modèle SpaCy pour le français\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "# Charger le texte\n",
    "n = 100000\n",
    "text = open(\"../data/tmp/N_texts.txt\", encoding='utf-8').read()[:n]\n",
    "\n",
    "# Traiter le texte\n",
    "doc = nlp(text)\n",
    "\n",
    "# Compter les entités\n",
    "people = defaultdict(int)\n",
    "locations = defaultdict(int)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"PER\" and len(ent.text) > 3:\n",
    "        people[ent.text] += 1\n",
    "    elif ent.label_ == \"LOC\":\n",
    "        locations[ent.text] += 1\n",
    "\n",
    "# Trier et imprimer les personnes\n",
    "sorted_people = sorted(people.items(), key=lambda kv: kv[1], reverse=True)\n",
    "print(\"\\nPersonnes les plus mentionnées :\")\n",
    "for person, freq in sorted_people[:50]:\n",
    "    print(f\"{person} apparait {freq} fois dans le corpus\")\n",
    "\n",
    "# Trier et imprimer les lieux\n",
    "sorted_locations = sorted(locations.items(), key=lambda kv: kv[1], reverse=True)\n",
    "print(\"\\nLieux les plus mentionnés :\")\n",
    "for location, freq in sorted_locations[:50]:\n",
    "    print(f\"{location} apparait {freq} fois dans le corpus\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation en phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fichiers d'inputs et d'outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = \"../data/tmp/texts.txt\"\n",
    "outfile = \"../data/tmp/regex/sen.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation en phrases du corpus complet et création d'un nouveau fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMIT = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(outfile, 'w', encoding=\"utf-8\") as output:\n",
    "    with open(infile, encoding=\"utf-8\", errors=\"backslashreplace\") as f:\n",
    "        content = f.readlines()\n",
    "        content = content[:LIMIT] if LIMIT is not None else content\n",
    "        n_lines = len(content)\n",
    "        for i, line in enumerate(content):\n",
    "            if i % 100 == 0:\n",
    "                print(f'processing line {i}/{n_lines}')\n",
    "            sentences = sent_tokenize(line)\n",
    "            for sent in sentences:\n",
    "                output.write(sent + \"\\n\")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word_Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# crreation d'un objet streame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySentences(object):\n",
    "    \"\"\"Tokenize and Lemmatize sentences\"\"\"\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "\n",
    "    def __iter__(self):\n",
    "        for line in open(self.filename, encoding='utf-8', errors=\"backslashreplace\"):\n",
    "            yield [unidecode(w.lower()) for w in wordpunct_tokenize(line)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = f\"../data/tmp/regex/sen.txt\"\n",
    "sentences = MySentences(infile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection des bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_phrases = Phrases(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(bigram_phrases.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bigram_phrases.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_ = list(bigram_phrases.vocab.keys())[144]\n",
    "print(key_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_phrases.vocab[key_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# conversion des Phrases en objet Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_phraser = Phraser(phrases_model=bigram_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_phrases = Phrases(bigram_phraser[sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_phraser = Phraser(phrases_model=trigram_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creation corpus avec unigrams, bi , tri "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(trigram_phraser[bigram_phraser[sentences]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir différentes valeurs pour window et min_count\n",
    "window_values = [3, 5, 7]\n",
    "min_count_values = [3, 5, 10]\n",
    "\n",
    "# Des essais-erreurs\n",
    "best_model = None\n",
    "best_score = float('-0.01')  \n",
    "\n",
    "for window_size in window_values:\n",
    "    for min_count_value in min_count_values:\n",
    "        print(f\"Entraînement avec window={window_size}, min_count={min_count_value}\")\n",
    "        \n",
    "        # Entraînement du modèle\n",
    "        model = Word2Vec(\n",
    "            corpus,\n",
    "            vector_size=32,\n",
    "            window=window_size,\n",
    "            min_count=min_count_value,\n",
    "            workers=4,\n",
    "            epochs=5\n",
    "        )\n",
    "\n",
    "        score = len(model.wv.key_to_index)\n",
    "\n",
    "        print(f\"Score obtenu : {score}\\n\")\n",
    "\n",
    "        # Comparaison avec le meilleur modèle précédent\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_model = model\n",
    "\n",
    "print(\"Entraînement terminé.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = f\"../data/newspapers_n.model\"\n",
    "model.save(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"../data/newspapers_n.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv[\"volleyball\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity(\"sc\", \"vainqueurs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(\"royal\", topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(\"champions\", topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.most_similar(positive=['volleyball', 'joueur'], negative=['equipe']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_words = model.wv.most_similar(\"volleyball\", topn=5)\n",
    "\n",
    "# Affichez les mots similaires\n",
    "for word, score in similar_words:\n",
    "    print(word, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.most_similar(positive=['volleyball', 'plage']))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recherche de termes liés au volleyball en compétition\n",
    "print(\"Termes liés au volleyball en compétition:\")\n",
    "print(model.wv.most_similar(positive=['volleyball', 'competition']))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trouver des termes associés au volleyball comme divertissement plutôt que fatigue \n",
    "print(\"Termes associés au volleyball comme divertissement:\")\n",
    "print(model.wv.most_similar(positive=['volleyball', 'divertissement'], negative=['fatigue']))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exploration des attributs de joueur, par exemple, la puissance dans le contexte du volleyball\n",
    "print(\"Exploration des attributs de joueur - Puissant vs Rapide:\")\n",
    "print(model.wv.most_similar(positive=['volleyball', 'puissant'], negative=['rapide']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tac_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
